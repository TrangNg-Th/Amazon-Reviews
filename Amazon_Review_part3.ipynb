{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Introduction**\r\n",
    "\r\n",
    "This is the last part of our journey to predict the rating for a given reviews. Using classification models on tfidf matrices doesn't allow us to explore the semantic relativities between words, which impacts the performance of the classification. We will in this section use embedded words instead of tfidf.\r\n",
    "\r\n",
    "## **Data preparation for sequence models**\r\n",
    "The main drawback of bag of words models is that they can't capture the order of the words in the setence. By using another representation of the document (this time, not by a bag of words for each document) but by a sequence of indices where each index points to a word in the vocabulary. \r\n",
    "For instant, the bag of words model can't not distinguish between 2 phrases :\r\n",
    "\r\n",
    "\"This is bad, I do not buy it again\"\r\n",
    "\r\n",
    "\"This is not bad, I do buy it again\"\r\n",
    "\r\n",
    "But sequence models can. In fact, by reprepsenting each document as a sequence of indices, we can capture the proximity between different words and there for can capture better the sentiment. \r\n",
    "The function below will take each feature ('Text_prep' or 'Summary_prep') and create a sequence of indices with max length of 300 words. We will again in this vectorization process, select the k most relevant words in the corpus.\r\n",
    "\r\n",
    "\r\n",
    "# Load packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import os  # Get directory\r\n",
    "\r\n",
    "# Print every output\r\n",
    "from IPython.core.interactiveshell import InteractiveShell\r\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
    "\r\n",
    "# Retrieve current directory\r\n",
    "repo = os.getcwd()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Load data\r\n",
    "x_train = pd.read_csv(repo+'/train.csv')\r\n",
    "y_train = pd.read_csv(repo+'/train_labels.csv')\r\n",
    "\r\n",
    "x_val = pd.read_csv(repo+'/valid.csv')\r\n",
    "y_val = pd.read_csv(repo+'/valid_labels.csv')  \r\n",
    "\r\n",
    "x_test = pd.read_csv(repo+'/test.csv')\r\n",
    "y_test = pd.read_csv(repo+'/test_labels.csv')  \r\n",
    "\r\n",
    "\r\n",
    "# Check data types\r\n",
    "x_train.head(3)\r\n",
    "print('\\n Shape of train data', x_train.shape)\r\n",
    "print('\\n Shape of validation data', x_val.shape)\r\n",
    "print('\\n Shape of test data', x_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary_prep</th>\n",
       "      <th>Text_prep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Impressive, but it is what it is</td>\n",
       "      <td>I agree that the shipping is too high.  A litt...</td>\n",
       "      <td>impressive</td>\n",
       "      <td>agree ship high little fishy taste prefer larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Itchy scratchy golden retriever no more</td>\n",
       "      <td>This food is the best.  Our golden retriever w...</td>\n",
       "      <td>itchy scratchy golden retriever</td>\n",
       "      <td>food best golden retriever prone sort allergie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perfect Every Cup</td>\n",
       "      <td>Since I must have my morning cappuccino it is ...</td>\n",
       "      <td>perfect every cup</td>\n",
       "      <td>since must morning cappuccino important coffee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Summary  \\\n",
       "0     Not Impressive, but it is what it is   \n",
       "1  Itchy scratchy golden retriever no more   \n",
       "2                        Perfect Every Cup   \n",
       "\n",
       "                                                Text  \\\n",
       "0  I agree that the shipping is too high.  A litt...   \n",
       "1  This food is the best.  Our golden retriever w...   \n",
       "2  Since I must have my morning cappuccino it is ...   \n",
       "\n",
       "                      Summary_prep  \\\n",
       "0                       impressive   \n",
       "1  itchy scratchy golden retriever   \n",
       "2                perfect every cup   \n",
       "\n",
       "                                           Text_prep  \n",
       "0  agree ship high little fishy taste prefer larg...  \n",
       "1  food best golden retriever prone sort allergie...  \n",
       "2  since must morning cappuccino important coffee...  "
      ]
     },
     "metadata": {},
     "execution_count": 29
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Shape of train data (36498, 4)\n",
      "\n",
      " Shape of validation data (9125, 4)\n",
      "\n",
      " Shape of test data (11406, 4)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we will use CNN model, we don't need to split data in to validation set. Therefore we can merge some dataframes together"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "x_train = pd.concat([x_train, x_val], ignore_index=True)\r\n",
    "y_train = pd.concat([y_train, y_val], ignore_index=True)\r\n",
    "print('\\n Shape of train data', x_train.shape)\r\n",
    "\r\n",
    "del x_val, y_val"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Shape of train data (45623, 4)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\r\n",
    "from tensorflow.python.keras.preprocessing import text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def sequence_vect(feature_to_be_sequenced, x_train, x_test, \r\n",
    "                top_frequent_words=20000, max_sequence_length=300):\r\n",
    "\r\n",
    "    \"\"\" This function transform the text into vector of indexes \r\n",
    "    IN : \r\n",
    "    feature_to_be_sequence : name of the col of df \r\n",
    "    x_train : dataframe of texts\r\n",
    "    x_test : dataframe of texts\r\n",
    "    top_frequent_words :\r\n",
    "    max_sequence_length :\r\n",
    "    \r\n",
    "    OUT : \r\n",
    "    \"\"\"\r\n",
    "   \r\n",
    "    # Extract the top_frequent_words in corpus\r\n",
    "    tokenizer = text.Tokenizer(num_words=top_frequent_words, # Number of wds to keep\r\n",
    "                               char_level=False, lower=False)\r\n",
    "    \r\n",
    "    # Fit on documents (create the mapping of words to integers)\r\n",
    "    tokenizer.fit_on_texts(x_train[feature_to_be_sequenced])\r\n",
    "\r\n",
    "    # Encode the reviews by the mapping of words to integers\r\n",
    "    x_train = tokenizer.texts_to_sequences(x_train[feature_to_be_sequenced])\r\n",
    "    x_test = tokenizer.texts_to_sequences(x_test[feature_to_be_sequenced])\r\n",
    "\r\n",
    "\r\n",
    "    # Pad length for shorter sequences in text or truncate for longer sequence in text\r\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_sequence_length)\r\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=max_sequence_length)\r\n",
    "    return(x_train, x_test, tokenizer.word_index)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usually for sequence models, there is an embedding layer in which the network \"learn\" how each sequence of indices (document) should be represented by a vector in a dense word vector space. However, it is usually faster to use already embedded vectors. We will check the GloVe embedded vectors that were trained from the Wikipedia corpus\r\n",
    "\r\n",
    "* Download if not yet\r\n",
    "* !wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
    "* !unzip -q glove.6B.zip\r\n",
    "\r\n",
    "\r\n",
    "Below is the function to import the 300 length Glove Vectors. To map them a chosen feature.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def map_GloVe_feature(feature, x_train, x_test, top_frequent_words):\r\n",
    "    \"\"\" This function will import the dictionary which maps each word in our dictionary to their numpy vect\r\n",
    "    representation \r\n",
    "    \"\"\"\r\n",
    "    path_glove = repo + '/glove.6B/glove.6B.300d.txt'\r\n",
    "    embedding_dim = 300\r\n",
    "    hits = 0\r\n",
    "    misses = 0\r\n",
    "\r\n",
    "    # Retrieve the dictionary of the given feature\r\n",
    "    x_train_emb, x_test_emb, dictionary_voc = sequence_vect(\r\n",
    "                                            feature_to_be_sequenced=feature,\r\n",
    "                                            max_sequence_length=300,\r\n",
    "                                            x_train=x_train, \r\n",
    "                                            x_test=x_test, \r\n",
    "                                            top_frequent_words=top_frequent_words)\r\n",
    "    \r\n",
    "    # Create a dictionary to \r\n",
    "    embeddings_index = {}\r\n",
    "    with open(path_glove, encoding='utf8') as f:\r\n",
    "        first_line = f.readline()\r\n",
    "        for line in f:\r\n",
    "            \r\n",
    "            # The first item is the word, the second item is 300D numpy vector\r\n",
    "            word, coefs = line.split(maxsplit=1)\r\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\r\n",
    "            embeddings_index[word] = coefs\r\n",
    "\r\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\r\n",
    "\r\n",
    "    vocabulary_size = len(dictionary_voc) + 1 # Number of words in vocabulary and 1 for unknown words\r\n",
    "    \r\n",
    "    # Prepare embedding matrix\r\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\r\n",
    "    for word, i in dictionary_voc.items():\r\n",
    "        embedding_vector = embeddings_index.get(word)\r\n",
    "        if embedding_vector is not None:\r\n",
    "            # Words not found in embedding index will be all-zeros.\r\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\r\n",
    "            embedding_matrix[i] = embedding_vector\r\n",
    "            hits += 1\r\n",
    "        else:\r\n",
    "            misses += 1\r\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\r\n",
    "    print(\"Ratio of words converted\", hits/(hits+misses))\r\n",
    "\r\n",
    "    return(x_train_emb, x_test_emb, embedding_matrix, vocabulary_size)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After different attempts with different number of most common words to match the vocabulary in the data with GloVe vector, we retain 20k most common words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print('For feature Summary_prep, using', 20000, 'most frequent words')\r\n",
    "embedded_summary, voc_size_summary = map_GloVe_feature('Summary_prep', x_train, x_test, top_frequent_words=20000)\r\n",
    "\r\n",
    "print('For feature Text_prep, using', 20000, 'most frequent words')\r\n",
    "embedded_text, voc_size_text = map_GloVe_feature('Text_prep', x_train, x_test, top_frequent_words=20000)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For feature Summary_prep, using 20000 most frequent words\n",
      "Found 400000 word vectors.\n",
      "Converted 7162 words (1979 misses)\n",
      "Ratio of words converted 0.7835028990263647\n",
      "For feature Text_prep, using 20000 most frequent words\n",
      "Found 400000 word vectors.\n",
      "Converted 21932 words (13842 misses)\n",
      "Ratio of words converted 0.6130709453793257\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "About 80% of our vocabulary is converted (for the feature Summary_prep), we can think of using the embedded vectors from GloVe. For the sake of curiosity, we will later use the same model but with a trainable Embedding layer.\r\n",
    "\r\n",
    "Next, we also remarked that we have more convertible words for the feature Summary_prep than the feature Text_prep, that's why we will use the feature Summary_prep in the following section.\r\n",
    "\r\n",
    "We will use the feature "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from tensorflow.keras.layers import Embedding, SeparableConv1D, Dense, Dropout, Input, MaxPooling1D, GlobalAveragePooling1D\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.backend import clear_session"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Clear the session before training model\r\n",
    "clear_session()\r\n",
    "\r\n",
    "# Define the constants\r\n",
    "vocabulary_size = voc_size_summary\r\n",
    "max_features = 20000\r\n",
    "embedding_dim = 300\r\n",
    "input_shape = x_train.shape[1:][0]\r\n",
    "embedding_matrix = embedded_summary\r\n",
    "\r\n",
    "# Modifiable hyper-params\r\n",
    "drop_out_rate=0.5 \r\n",
    "filters=64 \r\n",
    "kernel_size=5\r\n",
    "pool_size=3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# Dummify our labels so that we have 1 hot encoded matrix\r\n",
    "y_train_cat = pd.get_dummies(y_train.astype('str'), drop_first=False).astype(float)\r\n",
    "y_test_cat = pd.get_dummies(y_test.astype('str'), drop_first=False).astype(float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "\r\n",
    "See the difference between convolution and depthwise convolution [here](https://www.machinecurve.com/index.php/2019/09/24/creating-depthwise-separable-convolutions-in-keras/#a-brief-review-what-is-a-depthwise-separable-convolutional-layer>)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# MODEL WITH EMBEDDED GLOVE MATRIX\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(vocabulary_size, output_dim=embedding_dim, \r\n",
    "                    input_length=input_shape, weights=[embedding_matrix], \r\n",
    "                    trainable=False)) \r\n",
    "\r\n",
    "# Drop out some units\r\n",
    "model.add(Dropout(rate=drop_out_rate))\r\n",
    "\r\n",
    "# Add 2 layers of depthwise convolution \r\n",
    "model.add(SeparableConv1D(filters=filters, kernel_size=kernel_size,\r\n",
    "                          activation='relu', bias_initializer='random_uniform',\r\n",
    "                          depthwise_initializer='random_uniform',\r\n",
    "                          padding='same'))\r\n",
    "model.add(SeparableConv1D(filters=filters, kernel_size=kernel_size, \r\n",
    "                          activation='relu', bias_initializer='random_uniform',\r\n",
    "                          depthwise_initializer='random_uniform',\r\n",
    "                          padding='same'))\r\n",
    "# Add a Max pooling layer\r\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\r\n",
    "# Add 2 convolution layers\r\n",
    "model.add(SeparableConv1D(filters=filters*2, kernel_size=kernel_size,\r\n",
    "                          activation='relu', bias_initializer='random_uniform',\r\n",
    "                          depthwise_initializer='random_uniform',padding='same'))\r\n",
    "model.add(SeparableConv1D(filters=filters*2, kernel_size=kernel_size,\r\n",
    "                          activation='relu', bias_initializer='random_uniform',\r\n",
    "                          depthwise_initializer='random_uniform',padding='same'))\r\n",
    "model.add(GlobalAveragePooling1D())\r\n",
    "model.add(Dropout(rate=drop_out_rate))\r\n",
    "\r\n",
    "# Prediction layer\r\n",
    "model.add(Dense(5, activation=\"softmax\"))\r\n",
    "print(model.summary())\r\n",
    "\r\n",
    "# Compile the model\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \r\n",
    "              metrics=['accuracy'])\r\n",
    "\r\n",
    "\r\n",
    "# fit model\r\n",
    "history = model.fit(x=x_train, y=y_train_cat, workers=10, epochs=20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 4, 300)            2742600   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 300)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_16 (Separab (None, 4, 64)             20764     \n",
      "_________________________________________________________________\n",
      "separable_conv1d_17 (Separab (None, 4, 64)             4480      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_18 (Separab (None, 1, 128)            8640      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_19 (Separab (None, 1, 128)            17152     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,794,281\n",
      "Trainable params: 51,681\n",
      "Non-trainable params: 2,742,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node sequential_4/Cast (defined at \\AppData\\Local\\Temp/ipykernel_33828/4153739501.py:41) ]] [Op:__inference_train_function_5235]\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33828/4153739501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node sequential_4/Cast (defined at \\AppData\\Local\\Temp/ipykernel_33828/4153739501.py:41) ]] [Op:__inference_train_function_5235]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# evaluate the model\r\n",
    "_, train_acc = model.evaluate(x_train, y_train, verbose=1)\r\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=1)\r\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\r\n",
    "history.history\r\n",
    "\r\n",
    "# plot out the loss during training\r\n",
    "plt.figure(figsize=(10, 10))\r\n",
    "plt.title('Loss')\r\n",
    "plt.plot(history.history['loss'], label='train')\r\n",
    "plt.plot(history.history['val_loss'], label='validation')\r\n",
    "plt.legend()\r\n",
    "plt.show();\r\n",
    "\r\n",
    "# plot out accuracy during training\r\n",
    "plt.figure(figsize=(10, 10))\r\n",
    "plt.title('Accuracy')\r\n",
    "plt.plot(history.history['accuracy'], label='train')\r\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\r\n",
    "plt.legend()\r\n",
    "plt.show();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e88734949cfad01e9c9daf169f31362e2053d91cee716e8faab2b799a4f82d3c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}